<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <title>Madhav Agarwal</title>
    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet" />
    <link href="css/bootstrap-social.css" rel="stylesheet" />
    <link href="css/custom.css" rel="stylesheet" />
    <!-- Custom CSS -->
    <link href="css/modern-business.css" rel="stylesheet" />
    <!-- Custom Fonts -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css" />
    <link href=https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css rel="stylesheet"
        type="text/css" />
    <script src="js/jquery.js"></script>
    <script src="js/bootstrap.min.js"></script>

    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-115492969-1');



    </script>
</head>

<body>

    <!-- Page Content -->
    <div class="container pt-4">
        <!-- Marketing Icons Section -->

        <div class="row">
            <div class="col-sm-4 col-md-4">
                <img src="files/Profile.jpg" class="img-responsive img-rounded">
            </div>
            <div class="col-sm-8 col-md-8">
                <h3 style="color: rgb(45, 49, 66);">Madhav Agarwal</h3>
                <p>
                    I am an MS by Research student at <a href="http://cvit.iiit.ac.in/" target="_blank">CVIT, IIIT
                        Hyderabad.</a> I'm guided by <a href="https://faculty.iiit.ac.in/~jawahar/index.html"
                        target="_blank">Prof. C.V. Jawahar</a> and co-guided by <a href="https://vinaypn.github.io/"
                        target="_blank">Prof. Vinay P. Namboodiri</a>. My research interest lies in Computer Vision,
                    Multimodal Learning, Pattern Recognition and Machine Learning. My graduate research focuses on
                    Lip-Sync, Talking Head Generation and Face Reenactment along with their optimization for real-world
                    problems. I also have worked on the task of Table Detection in Document Images with high accuracy
                    under the guidance of Prof. C.V. Jawahar and Dr. Ajoy Mondal. Previously, I worked as a Data
                    Scientist with couple of start-ups, working broadly in the domain of Facial Recognition, Video
                    surveillance using AI, and Document Image Processing.
                </p>
                <p>
                    My work has been published in top computer vision and machine learning conferences.
                </p>
                <p>Contact: madhav
                    <coda>[dot] agarwal
                        <coda>[at] research
                            <coda>[dot] iiit
                                <coda>[dot] ac
                                    <coda>[dot] in</coda>
                                </coda>
                            </coda>
                </p>
                <p>
                <table>
                    <tr class="d-flex">
                        <td style="padding: 5px;">
                            <span style="font-size:24px"><a target="_blank"
                                    href='/files/MadhavAgarwalCV.pdf'>[CV]</a></span>

                        </td>
                        <td style="padding: 5px;">
                            <span style="font-size:24px"><a
                                    href='https://scholar.google.com/citations?user=t8VdoRYAAAAJ&hl=en'
                                    target="_blank">[Google Scholar]</a></span>

                        </td>
                        <td style="padding: 5px;">
                            <span style="font-size:24px"><a href='https://www.linkedin.com/in/madhav3101/'
                                    target="_blank">[LinkedIn]</a></span>

                        </td>
                        <td style="padding: 5px;">
                            <span style="font-size:24px"><a href='https://github.com/mdv3101'
                                    target="_blank">[GitHub]</a></span>

                        </td>
                        <td style="padding: 5px;">
                            <span style="font-size:24px"> <a
                                    href="mailto:madhav.agarwal@research.iiit.ac.in">[Mail]</a></span>

                        </td>
                        
                        </td>
                        <td style="padding: 5px;">
                            <span style="font-size:24px"><a href='https://arxiv.org/a/agarwal_m_1'
                                    target="_blank">[arXiv]</a></span>

                        </td>                        
                    </tr>
                </table>
            </div>
        </div>
        <hr>
        <div class="row">
            <h1> News: </h1>
        </div>
        <div class="row">
            <div class="col-sm-12 col-md-12">
		 <p><b>May 2023:</b> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320323003965" target="_blank">Dataset Agnostic Document Object Detection</a> got accepted to <a href="https://www.sciencedirect.com/journal/pattern-recognition"
                        target="_blank">Pattern Recognition</a> journal.</p>
		<p><b>May 2023:</b> Honoured to become a part of <a href="https://ellis.eu/" target="_blank">ELLIS PhD</a> program.</p>    
		<p><b>Apr 2023:</b> Awarded with <b>'Non-Academic Award'</b> by <a href="https://www.iiit.ac.in/" target="_blank">IIIT-Hyderabad</a> for contribution towards Mental Health on campus.</p>		
                <p><b>Sep 2022:</b> <a href="https://arxiv.org/abs/2210.03692" target="_blank">Compressing Video Calls using Synthetic Talking Heads</a> got accepted to <a href="https://bmvc2022.org/"
                        target="_blank">BMVC 2022</a>.</p>
                
                <p><b>Aug 2022:</b> <a href='https://arxiv.org/abs/2210.02755' target="_blank">Audio-Visual Face Reenactment</a> got accepted to <a href="https://wacv2023.thecvf.com/"
                        target="_blank">WACV 2023</a>.</p>

                <p> <b>Jan 2021:</b> Joined <a href="http://cvit.iiit.ac.in/" target="_blank">CVIT</a>, <a href="https://www.iiit.ac.in/" target="_blank">IIIT-Hyderabad</a> as a full-time MS by Research student under <a href="https://faculty.iiit.ac.in/~jawahar/index.html"
                        target="_blank">Prof. C.V. Jawahar</a>.</p>

                <p> <b>Oct 2020:</b> <a href="https://ieeexplore.ieee.org/abstract/document/9411922" target="_blank">CDeC-Net: Composite Deformable Cascade Network for Table Detection in Document Images</a> got accepted to <a
                        href="https://www.micc.unifi.it/icpr2020/" target="_blank">ICPR 2020</a> (<style="color:red"><b>Oral</b>).</p>

                <p><b> Nov 2019:</b> Joined <a href="https://www.iiit.ac.in/" target="_blank">IIIT-Hyderabad</a> as a Research Fellow at <a href="http://cvit.iiit.ac.in/" target="_blank">CVIT</a>.</p>
            </div>
        </div>
        <hr />
        <div class="row">
            <h1> Publications: </h1>
        </div>
        <br>
        <div class="row pubData">
            <div class="col-md-3 col-sm-3">
                <!-- <video class="img-responsive" loop autoplay muted>
                    <source src="files/pe.mp4" type="video/mp4">
                </video> -->
				<img class="img-responsive" src="files/avfr_architecture.png">
				<br>
                <img class="img-responsive" src="files/avfr_result.gif">
            </div>
            <div class="col-sm-9 col-md-9">
                <h3>Audio-Visual Face Reenactment</h3>
                <p>This work proposes a novel method to generate realistic talking head videos using audio and visual streams. 
                    We animate a source image by transferring head motion from a driving video using a dense motion field generated using learnable keypoints. 
                    We improve the quality of lip sync using audio as an additional input, helping the network to attend to the mouth region. 
                    We use additional priors using face segmentation and face mesh to improve the structure of the reconstructed faces. 
                    Finally, we improve the visual quality of the generations by incorporating a carefully designed identity-aware generator module. 
                    The identity-aware generator takes the source image and the warped motion features as input to generate a high-quality output with fine-grained details. 
                    Our method produces state-of-the-art results and generalizes well to unseen faces, languages, and voices. 
                    We comprehensively evaluate our approach using multiple metrics and outperforming the current techniques both qualitative and quantitatively. 
                    Our work opens up several applications, including enabling low bandwidth video calls.
                </p>
                <p> <b>Madhav Agarwal</b>, Rudrabha Mukhopadhyay, Vinay Namboodiri, C.V. Jawahar</p>
                <p>Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2023</p>
                <table>
                    <tr>
                        <td style="padding: 5px;">
                            <span style="font-size:24px"><a href='https://openaccess.thecvf.com/content/WACV2023/html/Agarwal_Audio-Visual_Face_Reenactment_WACV_2023_paper.html' target="_blank">[Conference Proceedings]</a></span>
                        </td>
                        <td style="padding: 5px;">
                            <span style="font-size:24px"><a href='https://arxiv.org/abs/2210.02755' target="_blank">[arXiv]</a></span>
                        </td>
                        <td style="padding: 5px;">
                            <span style="font-size:24px"><a href='http://cvit.iiit.ac.in/research/projects/cvit-projects/avfr' target="_blank">[Website]</a></span>
                        </td>
                        <td style="padding: 5px;">
                            <span style="font-size:24px"><a href='https://github.com/mdv3101/AVFR-Gan/' target="_blank">[Code]</a></span>
                        </td>
                </table>
            </div>
        </div>
        <hr>
        <div class="row pubData">
            <div class="col-md-3 col-sm-3">
                <!-- <video class="img-responsive" loop autoplay muted>
                    <source src="files/pe.mp4" type="video/mp4">
                </video> -->
				<img class="img-responsive" src="files/vc_compression_pipeline.png">
            </div>
            <div class="col-sm-9 col-md-9">
                <h3>Compressing Video Calls using Synthetic Talking Heads</h3>
                <p>We leverage the modern advancements in talking head generation to propose an end-to-end system for talking head video compression. 
				Our algorithm transmits pivot frames intermittently while the rest of the talking head video is generated by animating them. 
				We use a state-of-the-art face reenactment network to detect key points in the non-pivot frames and transmit them to the receiver. 
				A dense flow is then calculated to warp a pivot frame to reconstruct the non-pivot ones. 
				Transmitting key points instead of full frames leads to significant compression. 
				We propose a novel algorithm to adaptively select the best-suited pivot frames at regular intervals to provide a smooth experience. 
				We also propose a frame-interpolater at the receiver's end to improve the compression levels further. 
				Finally, a face enhancement network improves reconstruction quality, significantly improving several aspects like the sharpness of the generations. 
				We evaluate our method both qualitatively and quantitatively on benchmark datasets and compare it with multiple compression techniques.
                </p>
                <p> <b>Madhav Agarwal</b>, Anchit Gupta, Rudrabha Mukhopadhyay, Vinay Namboodiri, C.V. Jawahar</p>
                <p>33rd British Machine Vision Conference (<b>BMVC</b>), 2022</p>
                <table>
                    <tr>
                        <td style="padding: 5px;">
                            <span style="font-size:24px"><a href='https://bmvc2022.mpi-inf.mpg.de/289/' target="_blank">[Conference Proceedings]</a></span>
                        </td>
                        <td style="padding: 5px;">
                            <span style="font-size:24px"><a href='https://arxiv.org/abs/2210.03692' target="_blank">[arXiv]</a></span>

                        </td>
                        <td style="padding: 5px;">
                            <span style="font-size:24px"><a href='https://cvit.iiit.ac.in/research/projects/cvit-projects/talking-video-compression' target="_blank">[Website]</a></span>
                        </td>
                        
                </table>
            </div>
        </div>
        <hr>

        <div class="row pubData">
            <div class="col-md-3 col-sm-3">
                <!-- <video class="img-responsive" loop autoplay muted>
                    <source src="files/pe.mp4" type="video/mp4">
                </video> -->
                <img class="img-responsive" src="files/dolnet_architecture.jpg">
				
            </div>
            <div class="col-sm-9 col-md-9">
                <h3> Dataset Agnostic Document Object Detection</h3>
                <p>Localizing document objects such as tables, figures, and equations is a primary step for extracting information from document images. 
		We propose a novel end-to-end trainable deep network, termed Document Object Localization Network (dolnet), for detecting various objects present in the document images. 
		The proposed network is a multi-stage extension of Mask r-cnn with a dual backbone having deformable convolution for detecting document objects with high detection accuracy at a higher IoU threshold. 
		We also empirically evaluate the proposed dolnet on the publicly available benchmark datasets. 
		The proposed DOLNet achieves state-of-the-art performance for most of the bench-mark datasets under various existing experimental environments.

		Our solution has three important properties: 
			(i) a single trained model dolnet that performs well across all the popular benchmark datasets, 
			(ii) reports excellent performances across multiple, including with higher IoU thresholds, and 
			(iii) consistently demonstrate the superior quantitative performance by following the same protocol of the recent works for each of the benchmarks.
                </p>
                <p> Ajoy Mondal, <b>Madhav Agarwal</b>, C.V. Jawahar</p>
                <p> Pattern Recognition, Volume 142, 2023</p>
                <table>
                    <tr>
                        <td style="padding: 5px;">
                            <span style="font-size:24px"><a href='https://www.sciencedirect.com/science/article/abs/pii/S0031320323003965'
                                    target="_blank">[Journal]</a></span>

                        </td>               
                </table>
            </div>
        </div>
        <hr>
			
        <div class="row pubData">
            <div class="col-md-3 col-sm-3">
                <!-- <video class="img-responsive" loop autoplay muted>
                    <source src="files/pe.mp4" type="video/mp4">
                </video> -->
                <img class="img-responsive" src="files/cdecnet_architecture.jpeg">
				<br>
				<img class="img-responsive" src="files/cdecnet_results.JPG">
            </div>
            <div class="col-sm-9 col-md-9">
                <h3> CDeC-Net: Composite Deformable Cascade Network for Table Detection in Document Images</h3>
                <p>Localizing page elements/objects such as tables, figures, equations, etc. is the primary step in extracting information from document images. 
                    We propose a novel end-to-end trainable deep network, (CDeC-Net) for detecting tables present in the documents. 
                    The proposed network consists of a multistage extension of Mask R-CNN with a dual backbone 
                    having deformable convolution for detecting tables varying in scale with high detection accuracy at higher IoU threshold. 
                    We empirically evaluate CDeC-Net on the publicly available benchmark datasets with extensive experiments.
                    Our solution has three important properties: (i) a single trained model CDeC-Net‡ that performs well across all the 
                    popular benchmark datasets; (ii) we report excellent performances across multiple, including higher, thresholds of IoU; 
                    (iii) by following the same protocol of the recent papers for each of the benchmarks, we consistently demonstrate the superior quantitative performance. 
                    Our code and models are publicly available for enabling reproducibility of the results.
                </p>
                <p> <b>Madhav Agarwal</b>, Ajoy Mondal, C.V. Jawahar</p>
                <p>25th International Conference on Pattern Recognition (<b>ICPR</b>), 2020 (<style="color:red"><b>Oral</b>)</p>
                <table>
                    <tr>
                        <td style="padding: 5px;">
                            <span style="font-size:24px"><a href='https://ieeexplore.ieee.org/abstract/document/9411922'
                                    target="_blank">[Conference Proceedings]</a></span>

                        </td>               
                        <td style="padding: 5px;">
                            <span style="font-size:24px"><a href='https://arxiv.org/pdf/2008.10831.pdf'
                                    target="_blank">[arXiv]</a></span>

                        </td>
                        <td style="padding: 5px;">
                            <span style="font-size:24px"><a href='http://cvit.iiit.ac.in/usodi/cdec-net.php'
                                    target="_blank">[Website]</a></span>
                        </td>
                        <td style="padding: 5px;">
                            <span style="font-size:24px"><a href='https://github.com/mdv3101/CDeCNet'
                                    target="_blank">[Code]</a></span>
                        </td>
                </table>
            </div>
        </div>
        <hr>

    </div>
</body>

</html>
